library(tidyverse)
library(syuzhet)
library(tidytext)
library(sjPlot)
library(wordcloud)
library(readtext)
options(stringsAsFactors = F, # do not convert to factor upon loading
scipen = 999, # do not convert numbers to e-values
max.print = 200, # stop printing after 200 values
warn = -1) # as many warnings in R are useless (and annoying), you might want to disable them
theme_set(theme_sjplot2()) # set default ggplot theme to light
fs = 12 # default plot font size
corpus <- readtext("TS_corpus_txt", encoding = "UTF-8")  %>%
unnest_sentences(input = "text",
output = "sentence",
to_lower = F, drop = T) %>%
as_tibble()  %>%
group_by(doc_id) %>%
mutate(sentence_id = seq_along(sentence)) %>%
slice_sample(n = 150) %>% # random 150 sentences per text
ungroup() %>%
mutate(sentence = str_replace_all(sentence, pattern = "ſ", replacement = "s"))
View(corpus)
library(readxl)
corpus_meta <- read_excel(
# "D:/GitHub/zurich-sommerschule22.github.io/corpus/summer_school_corpus_info.xlsx"
"summer_school_corpus_info_new.xlsx"
) %>%
select(doc_id, title, author, pub_date, first_name, surname, collection)
corpus_meta <- readxl::read_xlsx(
# "D:/GitHub/zurich-sommerschule22.github.io/corpus/summer_school_corpus_info.xlsx"
"summer_school_corpus_info_new.xlsx"
)
readxl::read_excel(
# "D:/GitHub/zurich-sommerschule22.github.io/corpus/summer_school_corpus_info.xlsx"
"summer_school_corpus_info_new.xlsx"
)
corpus_meta <- readxl::read_excel(
# "D:/GitHub/zurich-sommerschule22.github.io/corpus/summer_school_corpus_info.xlsx"
"summer_school_corpus_info_new.xlsx"
)
corpus_token_SA <- readtext("TS_corpus_txt", encoding = "UTF-8")  %>%
unnest_sentences(input = "text",
output = "sentence",
to_lower = F, drop = T) %>%
as_tibble()  %>%
group_by(doc_id) %>%
mutate(sentence_id = seq_along(sentence)) %>%
slice_sample(n = 150) %>% # random 150 sentences per text
ungroup() %>%
mutate(sentence = str_replace_all(sentence, pattern = "ſ", replacement = "s")) %>%
left_join(read_excel(
# "D:/GitHub/zurich-sommerschule22.github.io/corpus/summer_school_corpus_info.xlsx"
"summer_school_corpus_info_new.xlsx"
) %>%
select(doc_id, title, author, pub_date, first_name, surname, collection)) %>%
filter(!is.na(author)) %>%
select(collection,
doc_id,
author,
title,
pub_date,
sentence,
sentence_id) %>%
unnest_tokens(input = "sentence", output = "token", to_lower = F, drop = F) %>%
group_by(title, sentence_id) %>%
mutate(token_id = seq_along(token)) %>%
ungroup() %>%
mutate(unique_word_id = seq_along(token)) %>%
left_join(read.csv("SA_resources/SentiArt.dat",
dec = ",",
encoding = "UTF-8") %>%
rename(token = 1) %>%
mutate(ang_z = (ang_z - mean(ang_z))/sd(ang_z)) %>%
select(-word)
)
stop_german <- tibble(word = stopwords::stopwords("de"))
stop_german2 <- stop_german
stop_german2$word <- str_to_sentence(stop_german2$word)
stop_german <- bind_rows(stop_german, stop_german2)
remove(stop_german2)
stop_german <- stop_german %>%
rename(token = word)
german_names <- read_delim("Vornamen_2020_Koeln_edited.csv",
delim = ";", escape_double = FALSE,
col_types = cols(anzahl = col_skip(),
position = col_skip()
),
trim_ws = TRUE) %>%
rename(first_name = vorname) %>%
rename(gender = geschlecht) %>%
distinct()
doubles_st <- german_names$first_name[duplicated(german_names$first_name)]
doubles_st <- german_names %>%
filter(first_name %in% doubles_st) %>%
distinct()
german_names <- german_names %>%
anti_join(doubles_st)
remove(doubles_st)
remove(corpus)
corpus_token_SA <- corpus_token_SA %>%
left_join(
german_names
)
View(corpus_meta)
View(corpus_token_SA)
View(corpus_meta)
corpus_token_SA <- readtext("TS_corpus_txt", encoding = "UTF-8")  %>%
unnest_sentences(input = "text",
output = "sentence",
to_lower = F, drop = T) %>%
as_tibble()  %>%
group_by(doc_id) %>%
mutate(sentence_id = seq_along(sentence)) %>%
slice_sample(n = 150) %>% # random 150 sentences per text
ungroup() %>%
mutate(sentence = str_replace_all(sentence, pattern = "ſ", replacement = "s")) %>%
left_join(read_excel(
# "D:/GitHub/zurich-sommerschule22.github.io/corpus/summer_school_corpus_info.xlsx"
"summer_school_corpus_info_new.xlsx"
) %>%
select(doc_id, title, author, pub_date, first_name, surname, collection)) %>%
filter(!is.na(author)) %>%
select(collection,
doc_id,
author,
title,
first_name,
pub_date,
sentence,
sentence_id) %>%
unnest_tokens(input = "sentence", output = "token", to_lower = F, drop = F) %>%
group_by(title, sentence_id) %>%
mutate(token_id = seq_along(token)) %>%
ungroup() %>%
mutate(unique_word_id = seq_along(token)) %>%
left_join(read.csv("SA_resources/SentiArt.dat",
dec = ",",
encoding = "UTF-8") %>%
rename(token = 1) %>%
mutate(ang_z = (ang_z - mean(ang_z))/sd(ang_z)) %>%
select(-word)
)
stop_german <- tibble(word = stopwords::stopwords("de"))
stop_german2 <- stop_german
stop_german2$word <- str_to_sentence(stop_german2$word)
stop_german <- bind_rows(stop_german, stop_german2)
remove(stop_german2)
stop_german <- stop_german %>%
rename(token = word)
german_names <- read_delim("Vornamen_2020_Koeln_edited.csv",
delim = ";", escape_double = FALSE,
col_types = cols(anzahl = col_skip(),
position = col_skip()
),
trim_ws = TRUE) %>%
rename(first_name = vorname) %>%
rename(gender = geschlecht) %>%
distinct()
doubles_st <- german_names$first_name[duplicated(german_names$first_name)]
doubles_st <- german_names %>%
filter(first_name %in% doubles_st) %>%
distinct()
german_names <- german_names %>%
anti_join(doubles_st)
remove(doubles_st)
corpus_token_SA <- corpus_token_SA %>%
left_join(
german_names
)
corpus_meta <- corpus_meta %>%
left_join(
german_names
)
corpus_token_SA_total <- corpus_token_SA %>%
select(doc_id, sentence_id, token_id) %>%
distinct() %>%
nrow()
female_total <- corpus_token_SA %>%
select(doc_id, sentence_id, gender, token_id) %>%
distinct() %>%
filter(gender == "w") %>%
nrow()
male_total <- corpus_token_SA %>%
select(doc_id, sentence_id, gender, token_id) %>%
distinct() %>%
filter(gender == "m") %>%
nrow()
corpus_meta %>%
select(author, gender) %>%
distinct() %>%
group_by(gender) %>%
count() %>%
ggplot(aes(y=n, x=gender, fill=gender, label=n)) +
geom_col() +
geom_text(nudge_y = 2)
View(corpus_meta)
corpus_token_SA %>%
filter(is.na(gender))
corpus_token_SA %>%
filter(is.na(gender)) %>%
select(author, first_name) %>%
distinct()
corpus_token_SA <- readtext("TS_corpus_txt", encoding = "UTF-8")  %>%
unnest_sentences(input = "text",
output = "sentence",
to_lower = F, drop = T) %>%
as_tibble()  %>%
group_by(doc_id) %>%
mutate(sentence_id = seq_along(sentence)) %>%
slice_sample(n = 150) %>% # random 150 sentences per text
ungroup() %>%
mutate(sentence = str_replace_all(sentence, pattern = "ſ", replacement = "s")) %>%
left_join(read_excel(
# "D:/GitHub/zurich-sommerschule22.github.io/corpus/summer_school_corpus_info.xlsx"
"summer_school_corpus_info_new.xlsx"
) %>%
select(doc_id, title, author, pub_date, first_name, surname, collection)) %>%
filter(!is.na(author)) %>%
select(collection,
doc_id,
author,
title,
first_name,
pub_date,
sentence,
sentence_id) %>%
unnest_tokens(input = "sentence", output = "token", to_lower = F, drop = F) %>%
group_by(title, sentence_id) %>%
mutate(token_id = seq_along(token)) %>%
ungroup() %>%
mutate(unique_word_id = seq_along(token)) %>%
left_join(read.csv("SA_resources/SentiArt.dat",
dec = ",",
encoding = "UTF-8") %>%
rename(token = 1) %>%
mutate(ang_z = (ang_z - mean(ang_z))/sd(ang_z)) %>%
select(-word)
)
stop_german <- tibble(word = stopwords::stopwords("de"))
stop_german2 <- stop_german
stop_german2$word <- str_to_sentence(stop_german2$word)
stop_german <- bind_rows(stop_german, stop_german2)
remove(stop_german2)
stop_german <- stop_german %>%
rename(token = word)
german_names <- read_delim("Vornamen_2020_Koeln_edited.csv",
delim = ";", escape_double = FALSE,
col_types = cols(anzahl = col_skip(),
position = col_skip()
),
trim_ws = TRUE) %>%
rename(first_name = vorname) %>%
rename(gender = geschlecht) %>%
distinct()
doubles_st <- german_names$first_name[duplicated(german_names$first_name)]
doubles_st <- german_names %>%
filter(first_name %in% doubles_st) %>%
distinct()
german_names <- german_names %>%
anti_join(doubles_st)
remove(doubles_st)
corpus_token_SA <- corpus_token_SA %>%
left_join(
german_names
)
corpus_meta <- corpus_meta %>%
left_join(
german_names
)
corpus_token_SA_total <- corpus_token_SA %>%
select(doc_id, sentence_id, token_id) %>%
distinct() %>%
nrow()
female_total <- corpus_token_SA %>%
select(doc_id, sentence_id, gender, token_id) %>%
distinct() %>%
filter(gender == "w") %>%
nrow()
male_total <- corpus_token_SA %>%
select(doc_id, sentence_id, gender, token_id) %>%
distinct() %>%
filter(gender == "m") %>%
nrow()
corpus_meta %>%
select(author, gender) %>%
distinct() %>%
group_by(gender) %>%
count() %>%
ggplot(aes(y=n, x=gender, fill=gender, label=n)) +
geom_col() +
geom_text(nudge_y = 2)
View(corpus_meta)
corpus_token_SA <- readtext("TS_corpus_txt", encoding = "UTF-8")  %>%
unnest_sentences(input = "text",
output = "sentence",
to_lower = F, drop = T) %>%
as_tibble()  %>%
group_by(doc_id) %>%
mutate(sentence_id = seq_along(sentence)) %>%
slice_sample(n = 150) %>% # random 150 sentences per text
ungroup() %>%
mutate(sentence = str_replace_all(sentence, pattern = "ſ", replacement = "s")) %>%
left_join(read_excel(
# "D:/GitHub/zurich-sommerschule22.github.io/corpus/summer_school_corpus_info.xlsx"
"summer_school_corpus_info_new.xlsx"
) %>%
select(doc_id, title, author, pub_date, first_name, surname, collection)) %>%
filter(!is.na(author)) %>%
select(collection,
doc_id,
author,
title,
first_name,
pub_date,
sentence,
sentence_id) %>%
unnest_tokens(input = "sentence", output = "token", to_lower = F, drop = F) %>%
group_by(title, sentence_id) %>%
mutate(token_id = seq_along(token)) %>%
ungroup() %>%
mutate(unique_word_id = seq_along(token)) %>%
left_join(read.csv("SA_resources/SentiArt.dat",
dec = ",",
encoding = "UTF-8") %>%
rename(token = 1) %>%
mutate(ang_z = (ang_z - mean(ang_z))/sd(ang_z)) %>%
select(-word)
)
stop_german <- tibble(word = stopwords::stopwords("de"))
stop_german2 <- stop_german
stop_german2$word <- str_to_sentence(stop_german2$word)
stop_german <- bind_rows(stop_german, stop_german2)
remove(stop_german2)
stop_german <- stop_german %>%
rename(token = word)
german_names <- read_delim("Vornamen_2020_Koeln_edited.csv",
delim = ";", escape_double = FALSE,
col_types = cols(anzahl = col_skip(),
position = col_skip()
),
trim_ws = TRUE) %>%
rename(first_name = vorname) %>%
rename(gender = geschlecht) %>%
distinct()
doubles_st <- german_names$first_name[duplicated(german_names$first_name)]
doubles_st <- german_names %>%
filter(first_name %in% doubles_st) %>%
distinct()
german_names <- german_names %>%
anti_join(doubles_st)
remove(doubles_st)
corpus_token_SA <- corpus_token_SA %>%
left_join(
german_names
)
remove(corpus_meta)
library(tidyverse)
library(readxl)
# PREP -----------
# if you erased your corpus, run this to recreate it:
corpus_token_SA <- readtext("TS_corpus_txt", encoding = "UTF-8")  %>%
unnest_sentences(input = "text",
output = "sentence",
to_lower = F, drop = T) %>%
as_tibble()  %>%
group_by(doc_id) %>%
mutate(sentence_id = seq_along(sentence)) %>%
slice_sample(n = 150) %>% # random 150 sentences per text
ungroup() %>%
mutate(sentence = str_replace_all(sentence, pattern = "ſ", replacement = "s")) %>%
left_join(read_excel(
# "D:/GitHub/zurich-sommerschule22.github.io/corpus/summer_school_corpus_info.xlsx"
"summer_school_corpus_info_new.xlsx"
) %>%
select(doc_id, title, author, pub_date, first_name, surname, collection)) %>%
filter(!is.na(author)) %>%
select(collection,
doc_id,
author,
title,
first_name,
pub_date,
sentence,
sentence_id) %>%
unnest_tokens(input = "sentence", output = "token", to_lower = F, drop = F) %>%
group_by(title, sentence_id) %>%
mutate(token_id = seq_along(token)) %>%
ungroup() %>%
mutate(unique_word_id = seq_along(token)) %>%
left_join(read.csv("SA_resources/SentiArt.dat",
dec = ",",
encoding = "UTF-8") %>%
rename(token = 1) %>%
mutate(ang_z = (ang_z - mean(ang_z))/sd(ang_z)) %>%
select(-word)
)
# stopwords list
stop_german <- tibble(word = stopwords::stopwords("de"))
stop_german2 <- stop_german
stop_german2$word <- str_to_sentence(stop_german2$word)
stop_german <- bind_rows(stop_german, stop_german2)
remove(stop_german2)
stop_german <- stop_german %>%
rename(token = word)
# proper names list
german_names <- read_delim("Vornamen_2020_Koeln_edited.csv",
delim = ";", escape_double = FALSE,
col_types = cols(anzahl = col_skip(),
position = col_skip()
),
trim_ws = TRUE) %>%
rename(first_name = vorname) %>%
rename(gender = geschlecht) %>%
distinct()
# let's make sure there are no double names
doubles_st <- german_names$first_name[duplicated(german_names$first_name)]
doubles_st <- german_names %>%
filter(first_name %in% doubles_st) %>%
distinct()
german_names <- german_names %>%
anti_join(doubles_st)
remove(doubles_st)
corpus_token_SA <- corpus_token_SA %>%
left_join(
german_names
)
corpus_meta <- corpus_meta %>%
left_join(
german_names
)
View(corpus_token_SA)
